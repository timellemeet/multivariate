{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "!activate PythonGPU\n",
    "import numpy as np\n",
    "from scipy.stats import skewnorm, skew\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error, classification_report, accuracy_score\n",
    "\n",
    "def simulate_data(classes, n_vars, n, max_mu, max_sigma, max_skew):\n",
    "    #The multivariate skew normal number generator\n",
    "    def rng(mu, sigma, skew, n=1):\n",
    "        k = len(mu)\n",
    "        if not (k == len(sigma) and k ==len(skew)): \n",
    "            raise Exception(\"Mu, Sigma and Skew should be same length\")\n",
    "\n",
    "        data = np.zeros((int(n),k))\n",
    "\n",
    "        for i in range(k):\n",
    "            data[:,i] = skewnorm.rvs(skew[i], loc=mu[i], scale=sigma[i], size=int(n)) \n",
    "\n",
    "        return data\n",
    "    \n",
    "    if(np.sum(classes) != 1):\n",
    "        raise Exception(\"Classes dont sum up to 1\")\n",
    "        \n",
    "    n_classes = len(classes)\n",
    "    sigma = np.random.randint(1,max_sigma,n_vars)\n",
    "    skew = np.random.randint(-max_skew,max_skew,n_vars)\n",
    "    mu =  np.random.randint(-max_mu, max_mu, (n_classes, n_vars))\n",
    "    \n",
    "    n_obs_class = np.round(np.dot(classes,n))\n",
    "    \n",
    "    data = np.zeros((int(np.sum(n_obs_class)),n_vars+1))\n",
    "    for i in range(n_classes):\n",
    "        #calculate indexes\n",
    "        start = int(np.sum(n_obs_class[0:i]))\n",
    "        end = int(np.sum(n_obs_class[0:i+1]))\n",
    "        \n",
    "        #set the data\n",
    "        data[start:end,0] = i\n",
    "        data[start:end,1:] = rng(mu[i,:], sigma, skew, n_obs_class[i])\n",
    "        \n",
    "    X = data[:,1:]\n",
    "    y = data[:,0]\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.33, \n",
    "    random_state=42,\n",
    "    stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def classify_lda(X_train, X_test, y_train, y_test, priors, plot=False):\n",
    "    lda = LinearDiscriminantAnalysis(priors=priors)\n",
    "    X_lda = lda.fit_transform(X_train, y_train)\n",
    "\n",
    "    predictions = lda.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"LDA Test accuracy \"+ str(accuracy))\n",
    "    print(predictions)\n",
    "\n",
    "    if plot:    \n",
    "        plt.xlabel('LD1')\n",
    "        plt.ylabel('LD2')\n",
    "        plt.scatter(\n",
    "            X_lda[:,0],\n",
    "            X_lda[:,1],\n",
    "            c=y_train,\n",
    "            cmap='Accent',\n",
    "        )\n",
    "        \n",
    "    return {\"method\": \"LDA\", \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": lda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quadratic\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def classify_qda(X_train, X_test, y_train, y_test, priors):\n",
    "    qda = QuadraticDiscriminantAnalysis(priors=priors)\n",
    "    X_qda = qda.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    predictions = qda.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"QDA Test accuracy \"+ str(accuracy))\n",
    "\n",
    "    return {\"method\": \"QDA\", \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": qda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def classify_logit(X_train, X_test, y_train, y_test):\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                             multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Logistic Test accuracy \"+ str(accuracy))\n",
    "    \n",
    "    return {\"method\": \"Logit\", \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": clf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def classify_knn(X_train, X_test, y_train, y_test, n_neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='euclidean')\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    predictions = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"KNN-\"+str(n_neighbors)+\" Test accuracy \"+ str(accuracy))\n",
    "    \n",
    "    return {\"method\": \"KNN-\"+str(n_neighbors), \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": knn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def classify_naivebayes(X_train, X_test, y_train, y_test, priors):\n",
    "    NB = GaussianNB(priors)\n",
    "    NB.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = NB.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"Naive Bayes Test accuracy \"+ str(accuracy))\n",
    "    \n",
    "    return {\"method\": \"Naive Bayes\", \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": NB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def classify_svm(X_train, X_test, y_train, y_test):\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"SVM Test accuracy \"+ str(accuracy))\n",
    "    \n",
    "    return {\"method\": \"SVM\", \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": svm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "def classify_neuralnet(X_train, X_test, y_train, y_test, n_vars, n_classes, depth=1, nodes=10, epochs=20):\n",
    "    inputs = keras.Input(shape=(n_vars,), name='obs')\n",
    "    x = layers.Dense(nodes, activation='relu')(inputs)\n",
    "    \n",
    "    if(depth>1):\n",
    "        for i in range(depth-1):\n",
    "            x = layers.Dense(nodes, activation='relu')(x)\n",
    "            \n",
    "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='Dataset')\n",
    "\n",
    "    display(model.summary())\n",
    "\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=64,\n",
    "                        epochs=epochs,\n",
    "                        validation_split=0.2)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    print(predictions)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Neural Network Test accuracy \"+ str(accuracy))\n",
    "    \n",
    "    return {\"method\": \"Net \"+\"-\".join([str(nodes) for i in range(depth)])+ \" E\"+str(epochs), \n",
    "            \"accuracy\": accuracy, \n",
    "            \"predictions\":predictions,\n",
    "            \"model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Test accuracy 0.5515151515151515\n",
      "[3. 2. 1. 0. 2. 1. 1. 2. 0. 1. 3. 0. 1. 2. 2. 2. 3. 1. 2. 1. 3. 2. 2. 3.\n",
      " 2. 3. 0. 2. 2. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 1. 1. 3. 1. 1. 1. 0. 3.\n",
      " 3. 0. 3. 2. 0. 3. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 3. 1. 3.\n",
      " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 2. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
      " 1. 0. 1. 1. 3. 3. 0. 3. 2. 0. 0. 3. 0. 2. 2. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
      " 0. 2. 3. 1. 1. 3. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
      " 1. 2. 0. 0. 1. 3. 3. 1. 1. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]\n",
      "Naive Bayes Test accuracy 0.5454545454545454\n",
      "SVM Test accuracy 0.5636363636363636\n",
      "Results after config 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >method</th>        <th class=\"col_heading level0 col1\" >accuracy</th>        <th class=\"col_heading level0 col2\" >predictions</th>        <th class=\"col_heading level0 col3\" >model</th>        <th class=\"col_heading level0 col4\" >config</th>        <th class=\"col_heading level0 col5\" >classes</th>        <th class=\"col_heading level0 col6\" >n_vars</th>        <th class=\"col_heading level0 col7\" >n</th>        <th class=\"col_heading level0 col8\" >max_mu</th>        <th class=\"col_heading level0 col9\" >max_sigma</th>        <th class=\"col_heading level0 col10\" >max_skew</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col0\" class=\"data row0 col0\" >SVM</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col1\" class=\"data row0 col1\" >56.364%</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col2\" class=\"data row0 col2\" >[3. 2. 1. 0. 2. 1. 1. 2. 0. 1. 3. 0. 1. 2. 2. 3. 3. 1. 2. 1. 3. 2. 2. 3.\n",
       " 3. 3. 3. 2. 2. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 1. 1. 3. 1. 1. 1. 0. 3.\n",
       " 3. 0. 3. 2. 0. 3. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 0. 1. 3.\n",
       " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 1. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
       " 1. 0. 1. 1. 3. 3. 0. 3. 2. 0. 0. 3. 0. 2. 2. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
       " 3. 2. 3. 1. 1. 1. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
       " 1. 2. 0. 0. 1. 3. 3. 1. 0. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col3\" class=\"data row0 col3\" >LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col5\" class=\"data row0 col5\" >[0.25, 0.25, 0.25, 0.25]</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col6\" class=\"data row0 col6\" >5</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col7\" class=\"data row0 col7\" >500</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col8\" class=\"data row0 col8\" >1</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col9\" class=\"data row0 col9\" >2</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row0_col10\" class=\"data row0 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col0\" class=\"data row1 col0\" >LDA</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col1\" class=\"data row1 col1\" >55.152%</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col2\" class=\"data row1 col2\" >[3. 2. 1. 0. 2. 1. 1. 2. 0. 1. 3. 0. 1. 2. 2. 2. 3. 1. 2. 1. 3. 2. 2. 3.\n",
       " 2. 3. 0. 2. 2. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 1. 1. 3. 1. 1. 1. 0. 3.\n",
       " 3. 0. 3. 2. 0. 3. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 3. 1. 3.\n",
       " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 2. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
       " 1. 0. 1. 1. 3. 3. 0. 3. 2. 0. 0. 3. 0. 2. 2. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
       " 0. 2. 3. 1. 1. 3. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
       " 1. 2. 0. 0. 1. 3. 3. 1. 1. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col3\" class=\"data row1 col3\" >LinearDiscriminantAnalysis(n_components=None, priors=[0.25, 0.25, 0.25, 0.25],\n",
       "              shrinkage=None, solver='svd', store_covariance=False,\n",
       "              tol=0.0001)</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col5\" class=\"data row1 col5\" >[0.25, 0.25, 0.25, 0.25]</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col6\" class=\"data row1 col6\" >5</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col7\" class=\"data row1 col7\" >500</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col8\" class=\"data row1 col8\" >1</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col9\" class=\"data row1 col9\" >2</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row1_col10\" class=\"data row1 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col0\" class=\"data row2 col0\" >Naive Bayes</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col1\" class=\"data row2 col1\" >54.545%</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col2\" class=\"data row2 col2\" >[3. 2. 1. 0. 2. 1. 2. 2. 0. 1. 3. 0. 1. 2. 2. 3. 3. 1. 2. 1. 3. 2. 2. 3.\n",
       " 3. 3. 0. 3. 1. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 2. 1. 3. 1. 1. 1. 0. 3.\n",
       " 3. 0. 3. 2. 0. 2. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 3. 2. 3.\n",
       " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 2. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
       " 1. 0. 1. 1. 3. 2. 0. 3. 2. 0. 0. 3. 0. 2. 1. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
       " 0. 2. 3. 1. 1. 3. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
       " 1. 2. 0. 1. 1. 3. 3. 1. 0. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col3\" class=\"data row2 col3\" >GaussianNB(priors=[0.25, 0.25, 0.25, 0.25], var_smoothing=1e-09)</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col5\" class=\"data row2 col5\" >[0.25, 0.25, 0.25, 0.25]</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col6\" class=\"data row2 col6\" >5</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col7\" class=\"data row2 col7\" >500</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col8\" class=\"data row2 col8\" >1</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col9\" class=\"data row2 col9\" >2</td>\n",
       "                        <td id=\"T_c58e5e06_1535_11ea_9101_9cb6d0e5d098row2_col10\" class=\"data row2 col10\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16afc20b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "timelabel =  datetime.now().strftime(\"%H-%M-%S - %d-%m-%Y\")\n",
    "\n",
    "\n",
    "configs = [{\n",
    "    \"classes\": [0.25, 0.25, 0.25, 0.25],\n",
    "    \"n_vars\": 5,\n",
    "    \"n\": 500,\n",
    "    \"max_mu\": 1,\n",
    "    \"max_sigma\": 2,\n",
    "    \"max_skew\": 1\n",
    "}] \n",
    "\n",
    "columns = ['method', 'accuracy','predictions', \"model\", \"config\"] + list(configs[0].keys())\n",
    "results = pd.DataFrame(columns=columns)\n",
    "results.style.format({\n",
    "    'accuracy': '{:,.3%}'.format\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "#run analysis\n",
    "for i, c in enumerate(configs):\n",
    "    X_train, X_test, y_train, y_test = simulate_data(c[\"classes\"], c[\"n_vars\"], c[\"n\"], c[\"max_mu\"], c[\"max_sigma\"], c[\"max_skew\"])\n",
    "    \n",
    "    lda = classify_lda(X_train, X_test, y_train, y_test, c[\"classes\"], False)                                     \n",
    "    results = results.append({**lda, **c, \"config\":i+1},ignore_index=True)\n",
    "    \n",
    "#     qda = classify_qda(X_train, X_test, y_train, y_test, c[\"classes\"])                                     \n",
    "#     results = results.append({**qda, **c, \"config\":i+1},ignore_index=True)\n",
    "    \n",
    "#     logit = classify_logit(X_train, X_test, y_train, y_test)                                     \n",
    "#     results = results.append({**logit, **c, \"config\":i+1},ignore_index=True)\n",
    "\n",
    "#     for k in [5,10,50,100]:\n",
    "#         knn = classify_knn(X_train, X_test, y_train, y_test, k)\n",
    "#         results = results.append({**knn, **c, \"config\":i+1},ignore_index=True\n",
    "\n",
    "    nb = classify_naivebayes(X_train, X_test, y_train, y_test, c[\"classes\"])\n",
    "    results = results.append({**nb, **c, \"config\":i+1},ignore_index=True)\n",
    "    \n",
    "    svm = classify_svm(X_train, X_test, y_train, y_test)\n",
    "    results = results.append({**svm, **c, \"config\":i+1},ignore_index=True)\n",
    "    \n",
    "#     for n in [{\"d\":2,\"n\":20, \"e\":1}]:\n",
    "#         neuralnet = classify_neuralnet(X_train, X_test, y_train, y_test, c[\"n_vars\"], len(c[\"classes\"]),  depth=n[\"d\"], nodes=n[\"n\"], epochs=n[\"e\"])                                 \n",
    "#         results = results.append({**neuralnet, **c, \"config\":i+1},ignore_index=True)\n",
    "    \n",
    "    print(\"Results after config \"+str(i+1))\n",
    "\n",
    "    results.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "        \n",
    "    display(results.style.format({\n",
    "    'accuracy': '{:,.3%}'.format\n",
    "    }))\n",
    "    \n",
    "    #saving results to file\n",
    "    results.to_pickle(\"./results/config \"+str(i+1)+\" of \"+str(len(configs))+\" \"+timelabel+\".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example to load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >method</th>        <th class=\"col_heading level0 col1\" >accuracy</th>        <th class=\"col_heading level0 col2\" >predictions</th>        <th class=\"col_heading level0 col3\" >model</th>        <th class=\"col_heading level0 col4\" >config</th>        <th class=\"col_heading level0 col5\" >classes</th>        <th class=\"col_heading level0 col6\" >n_vars</th>        <th class=\"col_heading level0 col7\" >n</th>        <th class=\"col_heading level0 col8\" >max_mu</th>        <th class=\"col_heading level0 col9\" >max_sigma</th>        <th class=\"col_heading level0 col10\" >max_skew</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col0\" class=\"data row0 col0\" >SVM</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col1\" class=\"data row0 col1\" >56.364%</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col2\" class=\"data row0 col2\" >[3. 2. 1. 0. 2. 1. 1. 2. 0. 1. 3. 0. 1. 2. 2. 3. 3. 1. 2. 1. 3. 2. 2. 3.\n",
       " 3. 3. 3. 2. 2. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 1. 1. 3. 1. 1. 1. 0. 3.\n",
       " 3. 0. 3. 2. 0. 3. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 0. 1. 3.\n",
       " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 1. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
       " 1. 0. 1. 1. 3. 3. 0. 3. 2. 0. 0. 3. 0. 2. 2. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
       " 3. 2. 3. 1. 1. 1. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
       " 1. 2. 0. 0. 1. 3. 3. 1. 0. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col3\" class=\"data row0 col3\" >LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col5\" class=\"data row0 col5\" >[0.25, 0.25, 0.25, 0.25]</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col6\" class=\"data row0 col6\" >5</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col7\" class=\"data row0 col7\" >500</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col8\" class=\"data row0 col8\" >1</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col9\" class=\"data row0 col9\" >2</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row0_col10\" class=\"data row0 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col0\" class=\"data row1 col0\" >LDA</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col1\" class=\"data row1 col1\" >55.152%</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col2\" class=\"data row1 col2\" >[3. 2. 1. 0. 2. 1. 1. 2. 0. 1. 3. 0. 1. 2. 2. 2. 3. 1. 2. 1. 3. 2. 2. 3.\n",
       " 2. 3. 0. 2. 2. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 1. 1. 3. 1. 1. 1. 0. 3.\n",
       " 3. 0. 3. 2. 0. 3. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 3. 1. 3.\n",
       " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 2. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
       " 1. 0. 1. 1. 3. 3. 0. 3. 2. 0. 0. 3. 0. 2. 2. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
       " 0. 2. 3. 1. 1. 3. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
       " 1. 2. 0. 0. 1. 3. 3. 1. 1. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col3\" class=\"data row1 col3\" >LinearDiscriminantAnalysis(n_components=None, priors=[0.25, 0.25, 0.25, 0.25],\n",
       "              shrinkage=None, solver='svd', store_covariance=False,\n",
       "              tol=0.0001)</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col4\" class=\"data row1 col4\" >1</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col5\" class=\"data row1 col5\" >[0.25, 0.25, 0.25, 0.25]</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col6\" class=\"data row1 col6\" >5</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col7\" class=\"data row1 col7\" >500</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col8\" class=\"data row1 col8\" >1</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col9\" class=\"data row1 col9\" >2</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row1_col10\" class=\"data row1 col10\" >1</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col0\" class=\"data row2 col0\" >Naive Bayes</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col1\" class=\"data row2 col1\" >54.545%</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col2\" class=\"data row2 col2\" >[3. 2. 1. 0. 2. 1. 2. 2. 0. 1. 3. 0. 1. 2. 2. 3. 3. 1. 2. 1. 3. 2. 2. 3.\n",
       " 3. 3. 0. 3. 1. 3. 2. 1. 2. 3. 1. 1. 0. 2. 3. 2. 2. 1. 3. 1. 1. 1. 0. 3.\n",
       " 3. 0. 3. 2. 0. 2. 1. 3. 3. 1. 0. 2. 0. 1. 0. 1. 1. 0. 2. 3. 1. 3. 2. 3.\n",
       " 0. 1. 2. 1. 2. 1. 1. 1. 2. 3. 1. 2. 2. 0. 1. 3. 0. 0. 2. 3. 1. 3. 2. 1.\n",
       " 1. 0. 1. 1. 3. 2. 0. 3. 2. 0. 0. 3. 0. 2. 1. 3. 2. 0. 2. 2. 3. 2. 2. 0.\n",
       " 0. 2. 3. 1. 1. 3. 1. 2. 1. 2. 2. 0. 2. 1. 3. 2. 2. 1. 2. 2. 2. 1. 0. 3.\n",
       " 1. 2. 0. 1. 1. 3. 3. 1. 0. 3. 0. 3. 3. 0. 3. 3. 2. 2. 1. 2. 2.]</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col3\" class=\"data row2 col3\" >GaussianNB(priors=[0.25, 0.25, 0.25, 0.25], var_smoothing=1e-09)</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col5\" class=\"data row2 col5\" >[0.25, 0.25, 0.25, 0.25]</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col6\" class=\"data row2 col6\" >5</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col7\" class=\"data row2 col7\" >500</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col8\" class=\"data row2 col8\" >1</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col9\" class=\"data row2 col9\" >2</td>\n",
       "                        <td id=\"T_266bc0a8_1536_11ea_953c_9cb6d0e5d098row2_col10\" class=\"data row2 col10\" >1</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x207f2333278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "old_results = pd.read_pickle(\"./results/config 1 of 1 19-58-47 - 02-12-2019.pkl\")\n",
    "\n",
    "display(old_results.style.format({'accuracy': '{:,.3%}'.format}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
